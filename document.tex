\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% read references.bib for the bibtex data
\bibliography{references}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (Literature Review)
    /Author (Gary Wang)
}

% set the title and author information
\title{Literature Review}
\author{Gary Wang}
\affiliation{Occidental College}
\email{}

\begin{document}

\maketitle




\section{Technical Background}

The application and importance of Image Based 3D reconstruction are numerous. The difficult task of 3D reconstruction facilitates a number of important technologies such as vr, autonomous vehicles, robotic navigation , 3D computer vision and animation. Its rapid growth as a research field underscores its significance in advancing modern technology.

3D gaussian splatting is an emerging method of representing radiance fields, gaining significant attention in the field of 3D graphics reconstruction and representation due to its real-time rendering capabilities, fast training times, and high fidelity and quality in output. To contextualize the incredible advancements this section briefly covers radiance fields and Nerfs before covering 3D gaussians.

Radiance fields are used to represent the distribution of light in a scene and can be separated into implicit and explicit representational methods\cite{chen2025survey3dgaussiansplatting}. 

Implicit radiance fields do not explicitly store the geometry or data of a 3D scene, instead the light distribution is implicitly represented. The most prominent method is Nerfs which are fully connected deep networks that take as input a 5d coordinate composed of x , y , z coordinates for position and two variables for viewing direction and output an opacity and RGB color value \cite{mildenhall2020nerfrepresentingscenesneural}. Although the output of Nerfs were of outstanding quality, training and rendering times were extremely high, optimizing for a single scene on a NVIDIA V100 GPU took at least one to two days.\cite{mildenhall2020nerfrepresentingscenesneural}. Although there were efforts to increase speed and reconstruction time such as TensoRF by Anpei Chen et al \cite{chen2022tensorftensorialradiancefields} which replaced the usage of MLP's with 4d tensors and matrix decomposition the speed and efficiency were still insufficient.

Explicit radiance fields directly represent the light distribution through explicit spatial structures such as a voxel grid or a set of points. For a set of points, each point needs to store its position and the radiance information which may change depending on viewing perspective. This allows for much faster access to the data but at the cost of requiring much higher memory usage , potentially lowering the feasible quality of the render. \cite{chen2025survey3dgaussiansplatting}.

3D gaussians lean more towards explicit radiance fields and has its roots in EWA splatting. Zwicker et al's\cite{zwicker2002ewa} 2002 paper EWA splatting provided a novel method to effectively derive EWA splat primitives (which are elliptical gaussians projected onto screens) for volume data and point sampled surface data. This framework allowed the rendering of high quality images while reducing aliasing artifacts which cause jagged edges and flickering in the image\cite{zwicker2002ewa}.

The 2023 paper 3D gaussian Splatting for Real-Time Radiance Field Rendering by Bernhard Kerbl is the foundational paper of 3D gaussian splatting. It brings three contributions. First the introduction of 3D gaussians as a flexible and expressive method of scene representation. Second, an optimization method that effects 3D position, opacity, anisotropic covariance (which dictates shape and orientation) and spherical harmonic coefficients (responsible for rgb color) as well as density control that involves splitting or cloning 3d gaussians. Third, a differentiation approach that allows real-time rendering due to the desirable properties of 3D gaussian representation\cite{kerbl20233dgaussiansplattingrealtime}. In the evaluation of this paper, running on the same hardware, both outputs were of similar quality; however, 3D gaussian splatting took only 35 to 45 minutes on average to train, while Nerfs had an average training time of 48 hours\cite{kerbl20233dgaussiansplattingrealtime}.

\section{Prior Work}

Many achievements have already been made in the field of 3D gaussian splatting with great focus on improving the already fast training and rendering time, as well as developing better optimizations for higher quality outputs.


Scaffold-GS by Tao lu et al aims at improving output quality by approaching the problem of redundant gaussians due to the original method ignoring underlying scene geometry. For example, when capturing the interior of a room walls are of uniform color and texture which does not require a high number of gaussians. They achieve this through three contributions. First leveraging anchor points which tether a group of gaussians in order to guide the local distribution of gaussians, anchor points are initialized as the centers of voxels in a voxel grid applied to the sparse point cloud. Second, within the camera view generate 3D gaussians on the fly from anchor points using neural networks where each parameter has its own MLP to help generate the parameters taking in as input the anchor feature, relative viewing distance an direction between anchor point and camera. Third, a more optimal culling and growing strategy is developed. Local gradients of neural gaussians are used to determine the creation of a new anchor point (high gradient). The culling of anchor points is determined by a cumulative opacity value of the anchors gaussians, if the opacity value of the gaussians are to low the anchor point is culled. The evaluation of this new approach shows it consistently outperformed standard methods performing better on thin geometries, fine scale details, texture-less areas and light effects\cite{lu2023scaffoldgsstructured3dgaussians}.

Mip splatting similarly focuses on improving the output images quality and accuracy but focusing more specifically on the effect of dilation due to zooming in. The work highlights how zooming in incorrectly enlarges 3D gaussians while zooming out shrinks the gaussians when projecting onto a 2D screen. The contributions of this work are a modified 3D smoothing filter and a 2D filter which reduces artifacts and aliasing when adjusting zoom\cite{yu2023mipsplattingaliasfree3dgaussian}.

Structure aware 3D gaussian like scaffold gs also focuses on the issue that the original method ignores underlying scene geometry. The paper highlights how this leads to floating points, reduced quality and expressivity of the output and artifacts. The approach to the issue is also very similar as graph neural networks are used to learn local global features for each point and MLP's are used to decode features into parameters for gaussians. The primary difference between these two approaches is one utilizes anchor points to represent a set of points while structure award 3Dgs utilizes a KNN to link points to from a point graph. The evaluation of this method illustrates outputs are of state of the art quality while reducing memory usage by up to 24x \cite{ververas2024sagsstructureaware3dgaussian}.

Turbo GS which builds upon scaffold GS and Mini splatting 2 focus on optimizing training speed. Turbo GS shows the original densification strategy which is based on increasing the number of gaussians in areas with strong positional gradients fails in texture-less regions where the gradient vanishes. The paper concludes after analyzing the effect of gradients from other parameters of 3D gaussians
that a combination of color and position gradients. Both these parameters complement each other as in flat and colorful areas position gradients fail but color gradients are effective, similarly in complex geometry but uniform color positional gradients compensate for the color gradient\cite{lu2024turbogsaccelerating3dgaussian}. Mini splatting 2 takes a different approach, identifying critical gaussians which are more likely to represent object surfaces and specifically aggressively densify these gaussians by cloning them. Critical gaussians are identified by having the maximum blending weight per viewing direction\cite{fang2024minisplatting2building360scenes}. Turbo GS additionally utilizes selective densification strategy that reduces the threshold number of times a critical gaussian can be visited by half if the number of times visited is above the current threshold\cite{lu2024turbogsaccelerating3dgaussian}. Mini splatting 2 instead incorporates additional culling strategies that uses visibility determined by accumulated blending weights to discard certain gaussians for more efficient rendering\cite{fang2024minisplatting2building360scenes}.

Other notable works include MV splat  and 2D gaussian splatting. Mv splat contributes a method to train 3D gaussians with sparse images while still providing clean outputs \cite{Chen_2024}.2D gaussian splatting presents a novel method which uses circular discs instead of 3d ellipses for better surface representation as discs can be aligned tangentially to surface planes\cite{Huang2DGS2024}.

\section{Methods}

The primary components of traditional 3Dgs include a sfm sparse point cloud initialization, an optimization step and rendering step. From the sparse point cloud 3D gaussians are initialized defined by a covariance, mean and opacity. Optimization is then applied to these parameters through gradient descent and during this process density control is also applied cloning or splitting gaussians.\cite{kerbl20233dgaussiansplattingrealtime} The rendering step is possible due to Zwicker et al's EWA splatting paper\cite{zwicker2002ewa} which provides a crucial equation where the covariance of the 3D gaussian can be mapped to 2D easily (where W is the camera transformation matrix and J is the jacobian matrix).
\[
\Sigma'_{\text{camera}} = J W \Sigma_{\text{world}} W^T J^T 
\]
A fast tile based rasterization that allows alpha blending of the splats is then applied to render the image in order to compute loss and update gradients\cite{kerbl20233dgaussiansplattingrealtime}. 

Turbo GS improves upon this framework by improving the initialization step and the densification strategy. As mentioned previously a better densification strategy using a combination of color and positional gradients is used to determine whether to split gaussians. Additionally a selective strategy is applied to densification where gaussians that are visited above a threshold number of times are eligible to spawn new gaussians and gaussians consistently below the threshold have it halved. The authors also constrain the maximum number of gaussians allowed. Through their investigations they found the relation between fitting iterations and model convergence follows a power function and so apply a power law based budget schedule to ensure a controlled convergence process. The authors optimize initialization by constructing a K dimensional tree with the initial sparse points and employ a NN method to generate additional points increasing the density of the point cloud. This provides a more effective foundation for training. Other optimizations are also applied in various areas such as during training only a subset of pixels in a chessboard pattern are rendered to increase speed while providing sufficient information. In a test case of rendering a bicycle turbo GS achieved a convergence time of 13 minutes compared to standard 3DGS which took 187 minutes\cite{lu2024turbogsaccelerating3dgaussian}. 


\section{Evaluation metrics}

Some of the most common full reference image measures in this field are SSIM, PSNR and LPIPs. PSNR is the most simple of the three and stands for peak signal to noise ratio, it is a measure related to mean squared error and its popularity stems from its simple calculations, clear physical meanings and easy mathematical optimization. However, due to its simplicity it cannot measure perceived visual quality which often focuses on structural aspects\cite{1284395}. An example of such a case would be the comparison between an image with gaussian noise applied and the same image but instead with blur applied. When comparing both images to the original picture it is likely the image with blur applied will have a higher PSNR score than the image with gaussian noise. This is because PSNR measures pixel wise differences and gaussian noise adds random differences while blurring only spreads the difference. However, the image with noise is likely of higher visual quality as the overall structure and edges are often preserved, but blurring the image destroys structural features.

SSIM tries to solve PSNR's failure to capture structural information and aptly stands for structural similarity index measure. The paper describes a new philosophy to approach the problem;. First rather than using perceived errors to quantify image degradation, it is treated as perceived structural changes instead. Second the original error sensitivity is a bottom up approach (looking at details then big picture) where as this new approach is top down. SSIM separates similarity measurement into three different and relatively independent components, luminesce, contrast and structure. Luminesce comparison is done with mean intensity, contrast with the standard deviation and structure is compared with the correlation coefficient which is the normalized covariance\cite{1284395}. 

Although more qualified as a measure than PSNR SSIM still fails in certain cases. The paper Understanding SSIM describes an extremely specific case where SSIM fails. The base picture is that of earth from the moon with the test picture being an almost identical image, except with the surface of the moon being of higher luminesce and the sentence "Houston we have a problem" hidden in the black area in a shade that makes it almost undetectable at a glance. The first difference is much more obvious to most people and the embedded phrase is almost invisible however SSIM gives a higher error to the phrase than the clear difference in luminesce\cite{nilsson2020understandingssim}.

The most popular measure which emulates HVS to an impressive level of fidelity is Learned Perceptual Image Patch Similarity (LPIPS). LPIPS is a direct result deep learning. Researchers found that the internal activations of high level classification networks without further calibration corresponds almost perfectly to human visual judgment. The authors notes even simple unsupervised networks with initialization using multi stage K means clustering outperform traditional measures like SSIM by a large margin\cite{zhang2018unreasonableeffectivenessdeepfeatures}.

Additional useful measures in this field are training time and memory and storage usage as a majority of the research is aimed towards faster algorithms and cheaper methods of achieving similar quality as shown in TurboGS\cite{lu2024turbogsaccelerating3dgaussian} and Structure aware GS\cite{ververas2024sagsstructureaware3dgaussian}.

In terms of training speed comparisons and standards turbo GS uses the MipNeRF-360 data set for evaluation. 3DGS which is the baseline takes on average 30 minutes, other methods like Mini splatting and EAGLES takes around 20 minutes. Turbo GS takes on average only 5 minutes\cite{lu2024turbogsaccelerating3dgaussian}. The paper reports that in terms of quality turboGS also consistently preserves finer high frequency details in regions such as grass, pavement and floor which is absent in other implementations. In the MipNeRF-360 data set turbo gs has the second lowest lpips score 0.210 , the lowest being 0.188 with MIP splatting\cite{lu2024turbogsaccelerating3dgaussian} (lower lpip score means greater similarity). Although turbo gs does not have the greatest memory requirement at 240MB compared to the lowest memory requirement of 48MB by instant-NGP it is still significantly better than a majority of implementations.

\printbibliography

\end{document}

